---
title: "Confidence Intervals"
teaching: 0
exercises: 0
questions:
- "What is a confidence interval?"
objectives:
- ""
- ""
- ""
keypoints:
- "."
- "."
- "."
- "."
source: Rmd
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As a reminder, this is the folder structure you should have in your class directory. And you should set up your working directory to the scripts folder.
You also want to load tidyverse.

```{r}
setwd("./scripts")
library(tidyverse)
set.seed(1)
```

## Confidence Intervals

With the advance of statistics during the 20th century, it became more and more popular to emphasize p-values and significance of results. In the 1980's scientists have started to move away from this dichotomous thinking and back to considering the actual results and the magnitude of the statistical effect rather than whether it passes a certain threshold of significance, which is after all rather arbitrary 
Whether an observed effect is statistically significant or not at a given p-value, does not necessarily mean that it is scientifically meaningful, and vice versa.
But, as we saw, observed effects can differ from one sample to another, which is why we call them random variables. Confidence intervals provide a way to quantify this variation and account for the uncertainty in our estimates.

Again, it's important to remember that everything we're talking about here pertains to any kind of effect, or statistic, that we estimate, not just the mean or difference in means.

#### Confidence Interval for Population Mean

We start by calculating CI for the mean estimate of one population - control female mice - and then we'll do it for the difference between two means.
Remember, in real life we never have the real population at hand, so for illustration purposes in this class we are pretending to have the population by designating a certain dataset as our "real" population. We're basically simulating a hypothetical population without actually simulating anything. 
```{r chowPop}
pheno <- read.csv("../data/mice_pheno.csv") # we read this in earlier
chowPopulation <- pheno %>% 
  filter(Sex=="F" & Diet=="chow") %>% 
  select(Bodyweight) %>% 
  unlist
```

The population mean <i>&mu;<sub>X</sub></i> is our parameter of interest 
here:

```{r muChow}
mu_chow <- mean(chowPopulation)
print(mu_chow)
```

We are interested in estimating this parameter based on a small sample that we draw from the theoretical population. Let's start with a sample of size 30:

```{r sampleChow}
N <- 30
chow <- sample(chowPopulation, N)
print(mean(chow))
```

Obviously, the sample mean isn't exactly the same as the population mean because we can never capture the full variation in the population with one small sample, and we can never have enough samples to get the exact true mean, that's why we call it a random variable. So we use the CI in order to quantify that uncertainty and work around it.

Since a sample size of 30 is reasonably large, we can use the CLT to calculate CI.

By convention, the confidence interval is usually defined as having probability 0.95 of falling on the parameter that we are estimating, in this case the population mean.

So we want to estimate the boundaries of this interval in terms of our variable. So what range of bodyweight values will capture the true mean 95% of the time? In other words, if we were to repeatedly sample 30 individuals from this population many many times, what range of values will capture the true mean in 95% of these iterations?

What CLT tells us is a couple of things:

First, we can assume the boundaries are normally distributed, and we know that 95% of the normal distribution will fall within about 2 standard deviations away from the mean in each direction.
You can see that by typing in R:
```{r conf_int}
pnorm(2) - pnorm(-2)
```
This give you the proportion of the normal distribution that lies between 2 sd above and below the mean (it's actually 1.96 sd)
```{r}
pnorm(2) - pnorm(-2)
```

Second, we can calculate the standard error of the mean by taking the SD of the sample and divide it by sqrt of N.
```{r}
se <- sd(chow)/sqrt(N)
print(se)
```
Remember the standard error is basically the standard deviation of the sample mean, and it's a way to estimate the variation in the mean that we expect to find in random samples.
 
So, putting these two things together, we can construct the interval by looking for the bodyweight values that fall two SEs below and above the sample mean.
The simplified version would be `c(mean(chow) - 2 * se, mean(chow) + 2 * se )`

To get the most accurate calculation, we can use the qnorm() function to calculate the values that capture 95% of the distribution.
For our one sample this would be:
```{r}
Q <- qnorm(1 - 0.05/2)
ci.lo <- mean(chow) - Q * se
ci.hi <- mean(chow) + Q * se

ci.lo < mu_chow
ci.hi > mu_chow
```
If we're lucky, this CI we calculated based on this one sample does indeed include our "true" population mean.

However, we can take another sample and we might not be so lucky. As we said earlier, the theory tells us that we will cover the true mean population only 95% of the time. 
We can illustrate this by taking many more samples (250 in this case) and compare to our "true" mean:
```{r}
Q <- qnorm(1 - 0.05/2)
B <- 250
N <- 30

plot(mean(chowPopulation) + c(-7,7), c(1,1), type="n",
     xlab="weight", ylab="interval", ylim=c(1,B))
abline(v=mean(chowPopulation))
for (i in 1:B) {
  chow <- sample(chowPopulation,N)
  se <- sd(chow)/sqrt(N)
  interval <- c(mean(chow) - Q * se, mean(chow) + Q * se)
  covered <- 
    mean(chowPopulation) <= interval[2] & mean(chowPopulation) >= interval[1]
  color <- ifelse(covered,1,2)
  lines(interval, c(i,i),col=color)
}
```

#### Small Sample Size and the CLT

So we saw above that for *N = 30*, the CLT works very well.  
However, if *N = 5*, do these confidence intervals work just as well? 

We can explore that by repeating the same simulation with N = 5:
```{r}
Q <- qnorm(1 - 0.05/2)
B <- 250
N <- 5

plot(mean(chowPopulation) + c(-7,7), c(1,1), type="n",
     xlab="weight", ylab="interval", ylim=c(1,B))
abline(v=mean(chowPopulation))
for (i in 1:B) {
  chow <- sample(chowPopulation,N)
  se <- sd(chow)/sqrt(N)
  interval <- c(mean(chow) - Q * se, mean(chow) + Q * se)
  covered <- 
    mean(chowPopulation) <= interval[2] & mean(chowPopulation) >= interval[1]
  color <- ifelse(covered,1,2)
  lines(interval, c(i,i),col=color)
}
```

Notice that we have many more intervals now that do not contain the "true" mean. Also, notice that the CIs are wider now than with N=30. This reflects the fact that with smaller sample size, the sampling error around the sample mean is larger, so the uncertainty around the estimated mean is greater.

In practice, this is happening because our assumption of normality is invalid with such a small sample size, so the calculation of Q using the qnorm() function is incorrect.
As we learned last lesson, with small sample sizes, it's better to use the t-distribution, which in practice means using the qt() function instead of qnorm().

```{r}
Q <- qt(1 - 0.05/2, df=4)
B <- 250
N <- 5

plot(mean(chowPopulation) + c(-7,7), c(1,1), type="n",
     xlab="weight", ylab="interval", ylim=c(1,B))
abline(v=mean(chowPopulation))
for (i in 1:B) {
  chow <- sample(chowPopulation,N)
  se <- sd(chow)/sqrt(N)
  interval <- c(mean(chow) - Q * se, mean(chow) + Q * se)
  covered <- 
    mean(chowPopulation) <= interval[2] & mean(chowPopulation) >= interval[1]
  color <- ifelse(covered,1,2)
  lines(interval, c(i,i),col=color)
}
```

Now the intervals are even wider (slightly), but they still cover the true mean much more often than with the normal distribution. This is because the t-distribution has fatter tails, so it accounts for larger sampling error when capturing the 95% of the distribution. 

#### Connection Between Confidence Intervals and p-values

Reporting the effect size and confidence interval has become more popular than p-value, and for good reasons, but we often still want to calculate p-value, and we can get that from the CIs as well.

So if we want to know for example if a difference between two groups is larger than zero, instead of estimating confidence interval for the mean of one group, we can estimate CI for the difference between the means of the two groups. If that interval does not include zero then we can say that the probability of getting an observed difference that much higher than zero just by chance, if the true difference was really zero, is less than 5%. So we can reject the null hypothesis that there is no difference between the means.

If we want to use the t-test to study the difference between two means, we're estimating the t statistic - a "T normalized" difference - instead of the simple difference between the means, and we're using the t-distribution instead of the normal distribution to calculate CI around the t statistic.

Using the `t.test` function - 
First we specify the control and treatment vectors:
```{r control_v_treatment}
control <- pheno %>% 
  filter(Sex=="F" & Diet=="chow") %>% 
  select(Bodyweight) %>% 
  unlist

treatment <- pheno %>% 
  filter(Sex=="F" & Diet=="hf") %>% 
  select(Bodyweight) %>% 
  unlist
```

Then we run the t.test function:
```{r}
t.test(treatment, control, conf.level=0.95)
```

That gives us the t statistic (the difference between the group means, normalized in a "t" way), the df is high because we have many individuals in this dataset, and p-value is very low. But even if we didn't have the exact p-value, just by looking at the 95% CI, we can see that it does not contain zero. We can reject the null hypothesis with confidence of at least 95%. The probability that we would be wrong to reject the null hypothesis here is less than 5%.

We can repeat this with an even wider CI - 99% - and still get a CI that doesn't include zero:
```{r}
t.test(treatment, control, conf.level=0.99)
```

Notes
-----------
Optional exercise: what happens if we permute the diet factor (we obliterate the difference between chow and hf).  
To Do suggestions:  
- change the object names to something more relevant (e.g., bodyweight instead of chow)
- simulate the theoretical population instead of designating a dataset as the "population"
